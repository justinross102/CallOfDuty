---
title: "STAT 486 Report"
author: "Justin Ross"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
#reticulate::py_install("seaborn")
#reticulate::py_install("matplotlib")
```

```{python}
import pandas as pd
import numpy as np
from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error
from sklearn.compose import ColumnTransformer
import seaborn as sns
import matplotlib.pyplot as plt
import os
os.chdir('/Users/justinross/Documents/BYU/stat486/CallofDuty')

# read in data
cod = pd.read_csv("cod.csv")
cod = cod.drop('name', axis=1)
print(cod)
```

#### Introduction

One of life's most desperately desired achievements is a win in Call of Duty. As such, our goal in this analysis is to determine the factors that contribute to success in online multiplayer matches. The data for our analysis was collected from [Kaggle](https://www.kaggle.com/datasets/aishahakami/call-of-duty-players), and contains a number of variables relating to player performance. 

```{python}
# feature engineering

# Calculate Accuracy by dividing hits by shots, handling division by zero
mask = cod['shots'] != 0  # Create a mask for non-zero shots
cod.loc[mask, 'Accuracy'] = cod['hits'] / cod['shots']
cod.loc[~mask, 'Accuracy'] = pd.NA  # Set Accuracy to pd.NA for zero shots

# Calculate Headshot Ratio by dividing headshots by kills, handling division by zero
mask = cod['kills'] != 0  # Create a mask for non-zero shots
cod.loc[mask, 'Headshot Ratio'] = cod['headshots'] / cod['kills']
cod.loc[~mask, 'Headshot Ratio'] = pd.NA  # Set Accuracy to pd.NA for zero shots

# Calculate Average Deaths per Game
mask = cod['gamesPlayed'] != 0  # Create a mask for non-zero gamesPlayed
cod.loc[mask, 'Average Deaths per Game'] = cod['deaths'] / cod['gamesPlayed']
cod.loc[~mask, 'Average Deaths per Game'] = pd.NA  # Set to pd.NA for zero gamesPlayed
```


#### Exploratory Data Analysis (EDA)
- Present summary statistics / plots to give an overview of the data
- Present any key findings from EDA
```{python, fig.align = 'center'}
sns.scatterplot(x = "kills", y = "wins", data = cod)
plt.show()

sns.scatterplot(x = "timePlayed", y = "wins", data = cod)
plt.show()


sns.histplot(x = "timePlayed", data=cod)

cod.loc[cod['wins'] != 0, 'wins'].mean()
cod.loc[cod['wins'] != 0, 'wins'].median()
cod.loc[cod['gamesPlayed'] != 0, 'gamesPlayed'].mean()
cod.loc[cod['gamesPlayed'] != 0, 'gamesPlayed'].median()
```

In our EDA we found that almost every explanatory variable had a positive and linear relationship with the number of wins. We also found several outliers caused by players with an excessive amount of playing time as well as players with limited playing time.

#### Methods (depending on the structure of your project, only some of the following are applicable): 
- Overview of feature engineering and how it improved your model (including any features from dimension reduction)
- Present a list or table of all models used. For each model, provide a very brief description (slightly longer if the model is not commonly known)
- Key hyperparameters explored
- High-level model results 

#### Discussion on Model Selection:
Without going into deep details, mention any patterns observed across models, such as:  

- Were tree-based models consistently outperforming linear models?
- Did ensemble models show significant promise over single models?
- Talk about major pitfalls or challenges faced with certain models, e.g., overfitting with a deep neural network, or convergence issues with a certain algorithm.
Briefly touch upon why certain models didn't make the cut. This doesn't have to be detailed but can include reasons like:
- Poor performance on validation data.
- Overfitting issues.
- Computationally too intensive for the marginal gain in accuracy.
- Difficulty in hyperparameter tuning.

#### Detailed Discussion on Best Model:
- Go in-depth into the model that performed the best
- Hyperparameter tuning
- Performance metrics
- SHAP and/or feature importance results including a few individual false positives/false negatives and true positives/true negatives (adapted to regression if applicable)
- Insights from cluster analysis or anomaly detection

#### Conclusion and Next Steps:
- Conclude by reinforcing the choice of the best model and its implications.
- Report or reinforce the answer/conclusion about your original problem statement
- Discuss any potential future steps or improvements that could be made, perhaps leveraging models or techniques not yet explored.




