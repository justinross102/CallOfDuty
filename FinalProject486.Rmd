---
title: "STAT 486 Report"
author: "Justin Ross"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
#reticulate::py_install("seaborn")
#reticulate::py_install("matplotlib")
```

```{python}
import pandas as pd
import numpy as np
from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import PolynomialFeatures, StandardScaler
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error
from sklearn.compose import ColumnTransformer
import seaborn as sns
import matplotlib.pyplot as plt
import os
os.chdir('/Users/justinross/Documents/BYU/stat486/CallofDuty')

# read in data
cod = pd.read_csv("cod.csv")
cod = cod.drop('name', axis=1)
print(cod)
```

#### Introduction

One of life's most desperately desired achievements is a win in Call of Duty. As such, our goal in this analysis is to determine the factors that contribute to success in online multiplayer matches. The data for our analysis was collected from [Kaggle](https://www.kaggle.com/datasets/aishahakami/call-of-duty-players), and contains a number of variables relating to player performance. 

```{python}
# feature engineering

# Calculate Accuracy by dividing hits by shots, handling division by zero
mask = cod['shots'] != 0  # Create a mask for non-zero shots
cod.loc[mask, 'Accuracy'] = cod['hits'] / cod['shots']
cod.loc[~mask, 'Accuracy'] = pd.NA  # Set Accuracy to pd.NA for zero shots

# Calculate Headshot Ratio by dividing headshots by kills, handling division by zero
mask = cod['kills'] != 0  # Create a mask for non-zero shots
cod.loc[mask, 'Headshot Ratio'] = cod['headshots'] / cod['kills']
cod.loc[~mask, 'Headshot Ratio'] = pd.NA  # Set Accuracy to pd.NA for zero shots

# Calculate Average Deaths per Game
mask = cod['gamesPlayed'] != 0  # Create a mask for non-zero gamesPlayed
cod.loc[mask, 'Average Deaths per Game'] = cod['deaths'] / cod['gamesPlayed']
cod.loc[~mask, 'Average Deaths per Game'] = pd.NA  # Set to pd.NA for zero gamesPlayed
```


#### Exploratory Data Analysis (EDA)
- Present summary statistics / plots to give an overview of the data
- Present any key findings from EDA
```{python, fig.align = 'center'}
sns.scatterplot(x = "kills", y = "wins", data = cod)
plt.show()

sns.scatterplot(x = "timePlayed", y = "wins", data = cod)
plt.show()


sns.histplot(x = "timePlayed", data=cod)

cod.loc[cod['wins'] != 0, 'wins'].mean()
cod.loc[cod['wins'] != 0, 'wins'].median()
cod.loc[cod['gamesPlayed'] != 0, 'gamesPlayed'].mean()
cod.loc[cod['gamesPlayed'] != 0, 'gamesPlayed'].median()
```

In our EDA we found that almost every explanatory variable had a positive and linear relationship with the number of wins. We also found several outliers caused by players with an excessive amount of playing time as well as players with limited playing time.

#### Methods (depending on the structure of your project, only some of the following are applicable): 
- Overview of feature engineering and how it improved your model (including any features from dimension reduction)
- Present a list or table of all models used. For each model, provide a very brief description (slightly longer if the model is not commonly known)

|Model       | Description | Hyperparameters|
|:-----------|:---------|:------------|
|Linear Regression | This does Linear Regression This does Linear Regression This does Linear Regression This does Linear Regression This does Linear Regression| hyperparameters |
|SVM   | This does SVM stuff| hyperparameters |


- Key hyperparameters explored
- High-level model results 

#### Discussion on Model Selection:

Without going into deep details, mention any patterns observed across models, such as:  

- Were tree-based models consistently outperforming linear models?
- Did ensemble models show significant promise over single models?
- Talk about major pitfalls or challenges faced with certain models, e.g., overfitting with a deep neural network, or convergence issues with a certain algorithm.
Briefly touch upon why certain models didn't make the cut. This doesn't have to be detailed but can include reasons like:
- Poor performance on validation data.
- Overfitting issues.
- Computationally too intensive for the marginal gain in accuracy.
- Difficulty in hyperparameter tuning.

To our surprise, tree based models did not consistently outperform linear models. We used a random forest to predict wins, but the best test RMSE that we obtained was higher than the RMSE that we obtained with a linear model.
	
Ensemble methods also did not show significant promise over single models. The test RMSE from the random forests and gradient boosting models performed about the same as single models like k-nearest neighbors and performed worse than linear regression.
	
The biggest challenge that we all faced with more complex models like random forests and gradient boosting was overfitting. We consistently obtained significantly higher training RMSE’s than testing RMSE’s which was a surprise. Even with extensive hyperparameter tuning on the number of trees, depth of trees, and number of features considered at each split, the testing RMSE never got super close to being less than or equal to the training RMSE. Hyperparameter tuning on the learning rate and number of estimators in the gradient boosting model yielded similar results to the random forest.
	
Gradient boosting and random forests are both robust machine learning methods and we are sure that with an even more thorough hyper parameter tuning process, we could have found a model that we were satisfied with that doesn’t overfit the data. However, we determined that the computation would be too intensive for the marginal gain in accuracy. Even though linear regression is a simple method, we are satisfied with the RMSE that it yielded and the short amount of time it took to run.

#### Detailed Discussion on Best Model:
- Go in-depth into the model that performed the best
- Hyperparameter tuning
- Performance metrics
- SHAP and/or feature importance results including a few individual false positives/false negatives and true positives/true negatives (adapted to regression if applicable)
- Insights from cluster analysis or anomaly detection

CLUSTERING	
A K-means clustering analysis offered interesting insights into player performance differences. It split players into three groups based off their stats: casual, elite, and regular.
	
The casual players have low average wins and kills. They also had lower prestige levels and play time, indicating that they are inexperienced and only play Call of Duty casually. 
	 
The elite players average high numbers of wins, kills, time played, and prestige. They invest a lot of time and effort into Call of Duty and reap the reward of wins because of it.
	
The regular players find themselves square in the middle of the casual and elite players. Their average wins, kills, time played, and prestige show that they are regularly engaged in Call of Duty and have strong skills and abilities, but they do not log enough play time to reach the elite status of the tier above them.

#### Conclusion and Next Steps:
- Conclude by reinforcing the choice of the best model and its implications.
- Report or reinforce the answer/conclusion about your original problem statement
- Discuss any potential future steps or improvements that could be made, perhaps leveraging models or techniques not yet explored.




